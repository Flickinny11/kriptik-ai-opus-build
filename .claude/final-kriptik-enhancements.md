# Final KripTik Enhancements - Unified Implementation Plan
## Ultra-Synthesis of VL-JEPA + Hyper-Thinking + Model Enhancements + KripTik Cloud

> **Vision**: Transform KripTik AI into the world's most capable AI builder by unifying three revolutionary capabilities into a single, synergistic architecture that amplifies each enhancement through strategic integration.
>
> **Date**: January 2, 2026
> **Status**: Ready for Implementation via Opus 4.5 in Cursor 2.2
> **Auth Protection**: All auth files are LOCKED per AUTH-IMMUTABLE-SPECIFICATION.md - DO NOT MODIFY

---

## EXECUTIVE SYNTHESIS

This unified plan integrates **three transformative implementation plans** into a cohesive architecture:

| Source Plan | Core Capability | Synergy Role |
|-------------|-----------------|--------------|
| **implementation-plan-kriptik-ultimate.md** | VL-JEPA semantic understanding + KripTik Cloud | The Foundation Layer - provides semantic embeddings that ALL other systems consume |
| **implementation-plan-hyper-thinking.md** | 6-phase cognitive enhancement pipeline | The Intelligence Amplifier - makes every AI call 30-50% better using semantic context |
| **implementation-plan-model-enhancements-jan2026.md** | Latest Anthropic/OpenAI API features | The Capability Backbone - provides the raw AI power that other systems orchestrate |

### The Compound Effect Formula

```
SYNERGY = VL-JEPA Semantics × Hyper-Thinking Cognition × Model Enhancements × KripTik Cloud
        = Understanding × Reasoning × Power × Deployment
        = Complete Production Apps from Single Prompt
```

---

## ARCHITECTURE SYNTHESIS

### How The Three Plans Enhance Each Other

```
┌─────────────────────────────────────────────────────────────────────────────────────────────┐
│                           UNIFIED KRIPTIK ENHANCEMENT ARCHITECTURE                           │
│                                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────────────────────┐│
│  │                              LAYER 1: SEMANTIC FOUNDATION                                ││
│  │                          (VL-JEPA from kriptik-ultimate.md)                              ││
│  │                                                                                          ││
│  │   User Input ──► VL-JEPA Intent Embedding ──► Semantic Understanding                    ││
│  │                         │                            │                                   ││
│  │                         ▼                            ▼                                   ││
│  │   ┌──────────────────────────────────────────────────────────────────────────┐          ││
│  │   │ FEEDS INTO: Hyper-Thinking (decomposition uses semantics)                │          ││
│  │   │ FEEDS INTO: Model Selection (complexity from semantic analysis)         │          ││
│  │   │ FEEDS INTO: Pattern Matching (embedding similarity for shortcuts)       │          ││
│  │   │ FEEDS INTO: Intent Satisfaction (semantic match vs. visual output)      │          ││
│  │   └──────────────────────────────────────────────────────────────────────────┘          ││
│  └─────────────────────────────────────────────────────────────────────────────────────────┘│
│                                              │                                               │
│                                              ▼                                               │
│  ┌─────────────────────────────────────────────────────────────────────────────────────────┐│
│  │                           LAYER 2: COGNITIVE ENHANCEMENT                                 ││
│  │                      (Hyper-Thinking from hyper-thinking.md)                             ││
│  │                                                                                          ││
│  │   Phase 1: DECOMPOSITION ──► Uses VL-JEPA semantic components                           ││
│  │   Phase 2: PRIOR KNOWLEDGE ──► Embedding-based pattern retrieval (faster)               ││
│  │   Phase 3: EXPLORATION ──► Multiple approaches with Interleaved Thinking                ││
│  │   Phase 4: CRITIQUE ──► Deliberative Alignment specs injection                          ││
│  │   Phase 5: SYNTHESIS ──► o3 xhigh effort for critical synthesis                         ││
│  │   Phase 6: VERIFICATION ──► VL-JEPA visual verification (50x faster)                    ││
│  │                                                                                          ││
│  │   ┌──────────────────────────────────────────────────────────────────────────┐          ││
│  │   │ ENHANCED BY: Extended Thinking (preserved blocks across phases)          │          ││
│  │   │ ENHANCED BY: Interleaved Thinking (reason between tool calls)            │          ││
│  │   │ ENHANCED BY: Responses API (reasoning summaries for transparency)        │          ││
│  │   └──────────────────────────────────────────────────────────────────────────┘          ││
│  └─────────────────────────────────────────────────────────────────────────────────────────┘│
│                                              │                                               │
│                                              ▼                                               │
│  ┌─────────────────────────────────────────────────────────────────────────────────────────┐│
│  │                           LAYER 3: API POWER BACKBONE                                    ││
│  │                    (Model Enhancements from jan2026.md)                                  ││
│  │                                                                                          ││
│  │   ┌─────────────┐    ┌─────────────┐    ┌─────────────┐    ┌─────────────┐              ││
│  │   │ Responses   │    │ Interleaved │    │   xhigh     │    │  Context    │              ││
│  │   │    API      │    │  Thinking   │    │   Effort    │    │ Compaction  │              ││
│  │   │  (OpenAI)   │    │ (Anthropic) │    │ (o3/o4-mini)│    │(GPT-5.2-Cdx)│              ││
│  │   └─────────────┘    └─────────────┘    └─────────────┘    └─────────────┘              ││
│  │                                                                                          ││
│  │   ┌──────────────────────────────────────────────────────────────────────────┐          ││
│  │   │ POWERS: Hyper-Thinking phases with maximum reasoning capability          │          ││
│  │   │ POWERS: Intent Lock with 64K extended thinking                           │          ││
│  │   │ POWERS: Error Escalation with preserved context                          │          ││
│  │   │ ENABLES: 24-hour builds via context compaction                           │          ││
│  │   └──────────────────────────────────────────────────────────────────────────┘          ││
│  └─────────────────────────────────────────────────────────────────────────────────────────┘│
│                                              │                                               │
│                                              ▼                                               │
│  ┌─────────────────────────────────────────────────────────────────────────────────────────┐│
│  │                           LAYER 4: PRODUCTION DEPLOYMENT                                 ││
│  │                       (KripTik Cloud from kriptik-ultimate.md)                           ││
│  │                                                                                          ││
│  │   Frontend: Vercel/Cloudflare ◄── Enhanced by Hyper-Thinking code quality              ││
│  │   Backend: AWS ECS Fargate ◄── Auto-configured by Hyper-Thinking architecture          ││
│  │   AI/ML: RunPod/Vast.ai GPU ◄── Model selection optimized by VL-JEPA complexity        ││
│  │   Integrations: Nango OAuth ◄── Integration selection via Hyper-Thinking analysis      ││
│  │                                                                                          ││
│  │   ┌──────────────────────────────────────────────────────────────────────────┐          ││
│  │   │ RESULT: Complete production apps, not just code                          │          ││
│  │   │ RESULT: Backends actually deployed and running                           │          ││
│  │   │ RESULT: AI models hosted and serving                                     │          ││
│  │   └──────────────────────────────────────────────────────────────────────────┘          ││
│  └─────────────────────────────────────────────────────────────────────────────────────────┘│
│                                              │                                               │
│                                              ▼                                               │
│  ┌─────────────────────────────────────────────────────────────────────────────────────────┐│
│  │                           LAYER 5: LEARNING & EVOLUTION                                  ││
│  │              (Component 28 Enhanced by ALL THREE PLANS)                                  ││
│  │                                                                                          ││
│  │   L1: Experience Capture ◄── VL-JEPA embeddings + Hyper-Thinking traces                ││
│  │   L2: AI Judgment ◄── Embedding comparison (50x faster) + Reasoning chains             ││
│  │   L3: Shadow Models ◄── Sentence Transformers v5 trained on semantic data             ││
│  │   L4: Pattern Library ◄── Qdrant embedding clusters for instant recall                 ││
│  │   L5: Evolution ◄── Hyper-Thinking pattern capture + Model performance data            ││
│  │                                                                                          ││
│  │   ┌──────────────────────────────────────────────────────────────────────────┐          ││
│  │   │ FLYWHEEL: Every build makes the next build better                        │          ││
│  │   │ FLYWHEEL: Hyper-Thinking patterns accelerate future Hyper-Thinking       │          ││
│  │   │ FLYWHEEL: VL-JEPA embeddings enable instant pattern matching             │          ││
│  │   └──────────────────────────────────────────────────────────────────────────┘          ││
│  └─────────────────────────────────────────────────────────────────────────────────────────┘│
└─────────────────────────────────────────────────────────────────────────────────────────────┘
```

---

## SYNERGY INTEGRATION POINTS

### Critical Cross-Plan Integrations

| Integration Point | Plans Involved | Synergy Effect |
|-------------------|----------------|----------------|
| **Semantic Decomposition** | VL-JEPA + Hyper-Thinking | Decomposition phase uses semantic embeddings instead of text analysis - 10x more accurate sub-problem identification |
| **Embedding Pattern Shortcuts** | VL-JEPA + Hyper-Thinking + Component 28 | Pattern matching via embedding similarity enables instant shortcuts, reducing Hyper-Thinking from 15s to 2s |
| **xhigh Effort Synthesis** | Model Enhancements + Hyper-Thinking | Synthesis phase uses o3 xhigh effort for critical decisions - 95% reasoning depth |
| **Interleaved Critique** | Model Enhancements + Hyper-Thinking | Adversarial critique uses Interleaved Thinking for multi-step tool-based verification |
| **Semantic Intent Satisfaction** | VL-JEPA + Model Enhancements | Phase 5 uses VL-JEPA embeddings + o3 xhigh for semantic match verification - cannot cheat |
| **Visual Verification Speedup** | VL-JEPA + Verification Swarm | 50-100x faster verification via embedding comparison vs. LLM analysis |
| **Deliberative Quality Specs** | Model Enhancements + Hyper-Thinking | Critique phase injects Deliberative Alignment specs for 8.7% → 0.3% problematic outputs |
| **Context Compaction + Learning** | Model Enhancements + Component 28 | 24-hour builds via GPT-5.2-Codex compaction while preserving learning data |
| **Cloud Deployment Intelligence** | KripTik Cloud + Hyper-Thinking | Infrastructure decisions use Hyper-Thinking for optimal provider selection |

---

## IMPLEMENTATION PHASES (UNIFIED ORDER)

### Phase 1: Foundation Layer (Week 1-2)
**Goal**: Establish the semantic understanding infrastructure that ALL other systems depend on.

```
Week 1:
├── Day 1-2: Qdrant vector database + Docker setup
├── Day 3-4: Embedding service infrastructure (Sentence Transformers + CLIP)
├── Day 5-6: Shadow data collection hooks throughout KripTik
└── Day 7: Database schema updates for embeddings + Hyper-Thinking

Week 2:
├── Day 1-2: VL-JEPA service wrapper (semantic understanding)
├── Day 3-4: Intent Lock enhancement with VL-JEPA embeddings
├── Day 5-6: Intent embedding generation + uncertainty quantification
└── Day 7: Qdrant integration tests + validation
```

### Phase 2: API Power Backbone (Week 3)
**Goal**: Implement the latest Anthropic/OpenAI capabilities that power everything else.

```
Week 3:
├── Day 1-2: OpenAI Responses API integration (reasoning summaries)
├── Day 3: Reasoning effort xhigh level for o3/o4-mini
├── Day 4-5: Interleaved Thinking Beta implementation
├── Day 6: Preserved Thinking Blocks across turns
└── Day 7: Model configuration updates (January 2026 IDs)
```

### Phase 3: Cognitive Enhancement (Week 4-5)
**Goal**: Implement Hyper-Thinking with semantic awareness from Phase 1 and API power from Phase 2.

```
Week 4:
├── Day 1-2: Hyper-Thinking Engine core + Mode Controller
├── Day 3-4: Trigger Classifier with semantic complexity analysis
├── Day 5-6: Phase 1-2 (Decomposition + Prior Knowledge Injection)
└── Day 7: Embedding-based pattern retrieval for Phase 2

Week 5:
├── Day 1-2: Phase 3 (Parallel Exploration with Interleaved Thinking)
├── Day 3-4: Phase 4 (Adversarial Critique with Deliberative Alignment)
├── Day 5: Phase 5 (Synthesis with xhigh effort)
├── Day 6: Phase 6 (VL-JEPA Verification Bridge)
└── Day 7: Build Loop integration + Shadow Mode activation
```

### Phase 4: Enhanced Verification (Week 6)
**Goal**: Supercharge the Verification Swarm with semantic capabilities.

```
Week 6:
├── Day 1-2: Visual Verifier with VL-JEPA (embedding comparison)
├── Day 3-4: Anti-Slop detection via embedding distance
├── Day 5: Intent Satisfaction with semantic matching
├── Day 6: Gap Closers VL-JEPA integration
└── Day 7: Performance validation (50x speedup target)
```

### Phase 5: Context & Memory Enhancement (Week 7-8)
**Goal**: Implement advanced memory systems that combine all three plans.

```
Week 7:
├── Day 1-2: Context Compaction service (24-hour builds)
├── Day 3-4: Tree of Clarifications for disambiguation
├── Day 5-6: SAMEP encrypted context sharing
└── Day 7: Deliberative Alignment quality specs

Week 8:
├── Day 1-3: Graphiti bi-temporal memory service
├── Day 4-5: Component 28 L1-L2 embedding enhancement
├── Day 6: Component 28 L4 embedding clustering
└── Day 7: Evolution flywheel updates
```

### Phase 6: KripTik Cloud (Week 9-10)
**Goal**: Deploy complete production stacks with intelligent infrastructure decisions.

```
Week 9:
├── Day 1-2: RunPod/Vast.ai serverless GPU integration
├── Day 3-4: AWS ECS Fargate container deployment
├── Day 5-6: Nango OAuth 500+ integrations
└── Day 7: Stack wiring + environment configuration

Week 10:
├── Day 1-2: MCP server infrastructure
├── Day 3-4: Hyper-Thinking infrastructure analysis integration
├── Day 5: Cost estimation + optimization
├── Day 6-7: End-to-end deployment testing
```

### Phase 7: Advanced Features & Polish (Week 11-12)
**Goal**: Complete advanced capabilities and production hardening.

```
Week 11:
├── Day 1-2: Clone Mode with VL-JEPA video understanding
├── Day 3-4: Image-to-Code enhancement
├── Day 5-6: Fix My App visual analysis
└── Day 7: Multi-agent semantic coordination

Week 12:
├── Day 1-2: Performance optimization
├── Day 3-4: Hyper-Thinking pattern export (portable data)
├── Day 5: UI enhancements for new features
├── Day 6: Documentation updates
└── Day 7: Production readiness validation
```

---

## UNIFIED NLP PROMPTS

Each prompt below is designed for **Claude Opus 4.5 in Cursor 2.2**. Execute in order within each phase.

---

### PHASE 1 PROMPTS: Foundation Layer

---

#### NLP PROMPT 1.1: Vector Database Infrastructure (Qdrant)

```
TASK: Create the Qdrant vector database infrastructure for KripTik AI's unified embedding storage.

CONTEXT:
This is the FOUNDATION for all three enhancement plans:
- VL-JEPA needs embedding storage for intent/visual understanding
- Hyper-Thinking needs embedding retrieval for pattern shortcuts
- Component 28 needs embedding clusters for learning

We use Qdrant (self-hosted via Docker) for flexibility and cost control.

REQUIREMENTS:

1. Create `server/src/services/embeddings/qdrant-client.ts`:
   - Initialize Qdrant client (use @qdrant/js-client-rest v1.12+)
   - Create collections for ALL embedding types (see below)
   - Implement CRUD operations for embeddings
   - Support batch upsert for efficiency
   - Add similarity search with filters
   - Implement embedding clustering for Component 28 L4

2. Create `server/src/services/embeddings/types.ts`:
   - IntentEmbedding interface (VL-JEPA)
   - VisualEmbedding interface (VL-JEPA)
   - CodePatternEmbedding interface (Component 28)
   - ErrorFixEmbedding interface (Component 28)
   - HyperThinkingChainEmbedding interface (Hyper-Thinking)
   - DecompositionPatternEmbedding interface (Hyper-Thinking)

3. Create `server/src/services/embeddings/index.ts`:
   - Export unified embedding service
   - Provide singleton access

4. Create/Update `docker-compose.yml`:
   - Add Qdrant service
   - Configure persistent storage volume
   - Set appropriate memory limits (2GB minimum)

5. Create initialization script `scripts/init-qdrant.ts`:
   - Create ALL collections on startup
   - Set up indexes for common filters
   - Verify connectivity

COLLECTION SCHEMAS (Create ALL):

```typescript
// 1. Intent embeddings (VL-JEPA)
{
  name: 'intent_embeddings',
  vectors: { size: 1024, distance: 'Cosine' },
  payload_schema: {
    projectId: 'keyword',
    userId: 'keyword',
    intentId: 'keyword',
    appSoul: 'keyword',
    uncertainty: 'float',
    timestamp: 'datetime',
  }
}

// 2. Visual embeddings (VL-JEPA)
{
  name: 'visual_embeddings',
  vectors: { size: 1024, distance: 'Cosine' },
  payload_schema: {
    projectId: 'keyword',
    componentPath: 'keyword',
    appSoul: 'keyword',
    designScore: 'float',
    antiSlopScore: 'float',
    timestamp: 'datetime',
  }
}

// 3. Code pattern embeddings (Component 28 + Hyper-Thinking)
{
  name: 'code_pattern_embeddings',
  vectors: { size: 768, distance: 'Cosine' },
  payload_schema: {
    category: 'keyword',
    patternId: 'keyword',
    taskType: 'keyword',
    successRate: 'float',
    usageCount: 'integer',
    isHyperThinkingPattern: 'bool',
  }
}

// 4. Error fix embeddings (Component 28)
{
  name: 'error_fix_embeddings',
  vectors: { size: 768, distance: 'Cosine' },
  payload_schema: {
    errorType: 'keyword',
    fixPatternId: 'keyword',
    successRate: 'float',
    escalationLevel: 'integer',
  }
}

// 5. Hyper-Thinking chain embeddings (Hyper-Thinking)
{
  name: 'hyper_thinking_chains',
  vectors: { size: 768, distance: 'Cosine' },
  payload_schema: {
    taskType: 'keyword',
    decisionType: 'keyword',
    qualityScore: 'float',
    modelUsed: 'keyword',
    phasesCaptured: 'keyword',
  }
}

// 6. Decomposition pattern embeddings (Hyper-Thinking shortcuts)
{
  name: 'decomposition_patterns',
  vectors: { size: 768, distance: 'Cosine' },
  payload_schema: {
    taskType: 'keyword',
    complexity: 'integer',
    confidence: 'float',
    successRate: 'float',
  }
}

// 7. Reasoning skeleton embeddings (Hyper-Thinking)
{
  name: 'reasoning_skeletons',
  vectors: { size: 768, distance: 'Cosine' },
  payload_schema: {
    taskType: 'keyword',
    decisionType: 'keyword',
    sourceModel: 'keyword',
    successRate: 'float',
  }
}
```

CRITICAL INTEGRATIONS:
- This enables Hyper-Thinking Phase 2 (Prior Knowledge) to do embedding retrieval instead of text search
- This enables VL-JEPA semantic verification in Phase 5 (Intent Satisfaction)
- This enables Component 28 L4 to use embedding clustering for pattern discovery

VALIDATION:
- npm run build must pass
- Test Qdrant connection locally
- Verify ALL 7 collections created
- Test similarity search returns relevant results

DO NOT:
- Use deprecated Qdrant client methods
- Store embeddings in SQLite (too slow for similarity search)
- Skip any collection (all are needed for full synergy)
- Modify any auth files
```

---

#### NLP PROMPT 1.2: Embedding Model Infrastructure

```
TASK: Create the embedding model infrastructure for generating embeddings across all three enhancement plans.

CONTEXT:
This service generates embeddings for:
1. VL-JEPA: Intent understanding (text) and visual understanding (images)
2. Hyper-Thinking: Task decomposition, reasoning chains, pattern matching
3. Component 28: Code patterns, error fixes, decision traces

REQUIREMENTS:

1. Create `server/src/services/embeddings/sentence-transformer-client.ts`:
   - Use HuggingFace Inference API for initial deployment
   - Support multiple models for different use cases:
     - `all-MiniLM-L6-v2` (384 dim, fast) - for quick Hyper-Thinking shortcuts
     - `all-mpnet-base-v2` (768 dim, quality) - for code patterns
     - `BAAI/bge-large-en-v1.5` (1024 dim, SOTA) - for intent embeddings
   - Implement text embedding generation
   - Implement code embedding generation (same models work)
   - Add Redis caching layer for repeated embeddings
   - Support batch embedding for efficiency (critical for shadow mode)

2. Create `server/src/services/embeddings/vision-embedding-client.ts`:
   - Use OpenAI CLIP API via OpenRouter initially
   - Implement image embedding from base64
   - Implement image embedding from URL
   - Add screenshot-to-embedding pipeline (for VL-JEPA verification)
   - Implement video frame embedding (for Clone Mode)

3. Create `server/src/services/embeddings/embedding-service.ts`:
   - Unified embedding service that routes to appropriate model
   - Input types: text, code, image, screenshot, video_frame
   - Handle errors gracefully with fallbacks
   - Add observability/logging with [Embedding] prefix
   - Support async batch processing (critical for Hyper-Thinking shadow mode)

4. Create `server/src/services/embeddings/embedding-cache.ts`:
   - Redis-based caching for embeddings
   - TTL of 24 hours for text embeddings
   - TTL of 1 hour for visual embeddings
   - Cache key includes model version for invalidation

5. Update `.env.example`:
   - HUGGINGFACE_API_KEY
   - EMBEDDING_MODEL (default: all-mpnet-base-v2)
   - VISION_EMBEDDING_MODEL (default: clip-vit-large-patch14)
   - EMBEDDING_CACHE_TTL_HOURS

API PATTERNS:

```typescript
// Text/Code embedding (for Hyper-Thinking + Component 28)
const embedding = await embeddingService.embedText(
  'Create a login form with email and password',
  { model: 'bge-large-en-v1.5', purpose: 'intent' }
);

// Code-specific embedding (for Component 28 patterns)
const codeEmbedding = await embeddingService.embedCode(
  codeSnippet,
  { language: 'typescript', model: 'all-mpnet-base-v2' }
);

// Image embedding (for VL-JEPA visual verification)
const visualEmbedding = await embeddingService.embedImage(
  base64Screenshot,
  { model: 'clip-vit-large-patch14' }
);

// Batch embedding (for Hyper-Thinking shadow mode)
const embeddings = await embeddingService.embedBatch([
  { type: 'text', content: 'prompt 1', purpose: 'decomposition' },
  { type: 'text', content: 'prompt 2', purpose: 'pattern' },
  { type: 'image', content: base64Image, purpose: 'verification' },
]);

// Async batch for shadow mode (non-blocking)
embeddingService.embedBatchAsync(items).then(results => {
  // Store for learning
});
```

CRITICAL INTEGRATIONS:
- Hyper-Thinking Phase 2 uses this for pattern retrieval
- VL-JEPA Intent Lock uses this for semantic embeddings
- Component 28 L1-L4 uses this for experience/pattern storage
- Shadow Mode uses batch async for non-blocking learning

VALIDATION:
- npm run build must pass
- Test text embedding with sample prompt
- Test image embedding with sample screenshot
- Verify embedding dimensions match collection schemas
- Test batch processing doesn't block

DO NOT:
- Run Sentence Transformers locally (use API for now)
- Skip caching (embeddings are expensive)
- Block on async batch operations
- Modify any auth files
```

---

#### NLP PROMPT 1.3: Unified Database Schema Updates

```
TASK: Update the database schema to support ALL THREE enhancement plans with unified tables.

CONTEXT:
This schema update supports:
1. VL-JEPA: Embedding references for intents and visuals
2. Hyper-Thinking: Run tracking, patterns, skeletons, critiques
3. Model Enhancements: Reasoning metadata, context compaction history

CRITICAL: These tables must be designed for CROSS-PLAN SYNERGY.

REQUIREMENTS:

1. Update `server/src/schema.ts` with new tables and columns:

```typescript
// =============================================================================
// VL-JEPA TABLES (from kriptik-ultimate.md)
// =============================================================================

// VL-JEPA embedding metadata
export const vljepaEmbeddings = sqliteTable('vljepa_embeddings', {
  id: text('id').primaryKey().$defaultFn(() => crypto.randomUUID()),
  qdrantPointId: text('qdrant_point_id').notNull(),
  collectionName: text('collection_name').notNull(),
  sourceType: text('source_type').notNull(),  // 'intent', 'screenshot', 'video_frame'
  sourceId: text('source_id'),
  projectId: text('project_id'),
  uncertainty: real('uncertainty'),  // VL-JEPA confidence
  embeddingDimension: integer('embedding_dimension'),
  modelVersion: text('model_version'),
  createdAt: text('created_at').default(sql`(datetime('now'))`).notNull(),
});

// =============================================================================
// HYPER-THINKING TABLES (from hyper-thinking.md)
// =============================================================================

// Track every Hyper-Thinking run
export const hyperThinkingRuns = sqliteTable('hyper_thinking_runs', {
  id: text('id').primaryKey().$defaultFn(() => crypto.randomUUID()),
  runId: text('run_id').unique().notNull(),
  buildId: text('build_id'),
  projectId: text('project_id'),
  userId: text('user_id').notNull(),
  taskType: text('task_type').notNull(),
  taskHash: text('task_hash'),

  // Configuration
  mode: text('mode').notNull(),  // 'shadow', 'active', 'accelerated'
  modelUsed: text('model_used').notNull(),
  depth: text('depth').notNull(),  // 'light', 'standard', 'deep'

  // Results (JSON)
  phasesRun: text('phases_run'),
  decomposition: text('decomposition'),
  explorations: text('explorations'),
  critiques: text('critiques'),
  synthesisResult: text('synthesis_result'),

  // SYNERGY: VL-JEPA embedding references
  intentEmbeddingId: text('intent_embedding_id'),  // Links to vljepaEmbeddings
  decompositionEmbeddingId: text('decomposition_embedding_id'),  // For pattern matching

  // Metrics
  tokensUsed: integer('tokens_used'),
  durationMs: integer('duration_ms'),
  qualityScore: real('quality_score'),
  qualityImprovement: real('quality_improvement'),

  // Shadow mode comparison
  shadowRun: integer('shadow_run', { mode: 'boolean' }).default(false),
  rawOutput: text('raw_output'),
  rawQualityScore: real('raw_quality_score'),

  createdAt: text('created_at').default(sql`(datetime('now'))`).notNull(),
});

// Learned decomposition patterns (with embedding reference)
export const hyperThinkingDecompositionPatterns = sqliteTable('hyper_thinking_decomposition_patterns', {
  id: text('id').primaryKey().$defaultFn(() => crypto.randomUUID()),
  taskType: text('task_type').notNull(),
  taskSignature: text('task_signature'),
  decompositionTemplate: text('decomposition_template').notNull(),
  subProblemTypes: text('sub_problem_types').notNull(),

  // SYNERGY: Qdrant embedding ID for instant retrieval
  embeddingId: text('embedding_id'),  // Qdrant point ID in decomposition_patterns collection

  timesUsed: integer('times_used').default(0),
  successCount: integer('success_count').default(0),
  avgQualityScore: real('avg_quality_score'),
  avgQualityImprovement: real('avg_quality_improvement'),
  confidence: real('confidence').default(0.5),

  createdAt: text('created_at').default(sql`(datetime('now'))`).notNull(),
  updatedAt: text('updated_at').default(sql`(datetime('now'))`).notNull(),
});

// Effective adversarial critique questions
export const hyperThinkingCritiqueLibrary = sqliteTable('hyper_thinking_critique_library', {
  id: text('id').primaryKey().$defaultFn(() => crypto.randomUUID()),
  taskType: text('task_type').notNull(),
  critiqueQuestion: text('critique_question').notNull(),
  catchesCategory: text('catches_category'),
  exampleFinding: text('example_finding'),

  // SYNERGY: Deliberative Alignment integration
  alignsWithSpec: text('aligns_with_spec'),  // Which quality spec this enforces

  embeddingId: text('embedding_id'),
  timesAsked: integer('times_asked').default(0),
  timesLedToChange: integer('times_led_to_change').default(0),
  avgQualityImpact: real('avg_quality_impact'),
  falsePositiveRate: real('false_positive_rate').default(0),

  createdAt: text('created_at').default(sql`(datetime('now'))`).notNull(),
  updatedAt: text('updated_at').default(sql`(datetime('now'))`).notNull(),
});

// Reasoning skeletons (how Opus thinks)
export const hyperThinkingSkeletons = sqliteTable('hyper_thinking_skeletons', {
  id: text('id').primaryKey().$defaultFn(() => crypto.randomUUID()),
  taskType: text('task_type').notNull(),
  decisionType: text('decision_type'),
  skeletonTemplate: text('skeleton_template').notNull(),
  thinkingSteps: text('thinking_steps').notNull(),
  qualityIndicators: text('quality_indicators'),
  antiPatterns: text('anti_patterns'),

  // SYNERGY: Extended Thinking metadata
  thinkingBudgetUsed: integer('thinking_budget_used'),
  preservedBlocksCount: integer('preserved_blocks_count'),

  sourceModel: text('source_model'),
  sourceRunId: text('source_run_id'),
  embeddingId: text('embedding_id'),
  timesUsed: integer('times_used').default(0),
  successRate: real('success_rate').default(0.5),
  avgQualityScore: real('avg_quality_score'),

  createdAt: text('created_at').default(sql`(datetime('now'))`).notNull(),
  updatedAt: text('updated_at').default(sql`(datetime('now'))`).notNull(),
});

// Model performance with Hyper-Thinking (for equalization)
export const hyperThinkingModelPerformance = sqliteTable('hyper_thinking_model_performance', {
  id: text('id').primaryKey().$defaultFn(() => crypto.randomUUID()),
  model: text('model').notNull(),
  taskType: text('task_type').notNull(),
  withHyperThinking: integer('with_hyper_thinking', { mode: 'boolean' }).notNull(),

  runs: integer('runs').default(0),
  avgQualityScore: real('avg_quality_score'),
  avgTokensUsed: real('avg_tokens_used'),
  avgDurationMs: real('avg_duration_ms'),
  qualityPerToken: real('quality_per_token'),
  qualityPerSecond: real('quality_per_second'),

  optimalCompensationLevel: text('optimal_compensation_level'),
  optimalStructureDepth: text('optimal_structure_depth'),
  patternDependencyScore: real('pattern_dependency_score'),

  // SYNERGY: Effort level effectiveness
  bestEffortLevel: text('best_effort_level'),  // 'low', 'medium', 'high', 'xhigh'

  createdAt: text('created_at').default(sql`(datetime('now'))`).notNull(),
  updatedAt: text('updated_at').default(sql`(datetime('now'))`).notNull(),
});

// =============================================================================
// MODEL ENHANCEMENTS TABLES (from jan2026.md)
// =============================================================================

// Context compaction history
export const contextCompactionHistory = sqliteTable('context_compaction_history', {
  id: text('id').primaryKey().$defaultFn(() => crypto.randomUUID()),
  sessionId: text('session_id').notNull(),
  projectId: text('project_id'),

  originalTokens: integer('original_tokens').notNull(),
  compactedTokens: integer('compacted_tokens').notNull(),
  compressionRatio: real('compression_ratio'),

  summaryHash: text('summary_hash'),  // For recovery
  preservedContextKeys: text('preserved_context_keys'),  // JSON array

  compactedAt: text('compacted_at').default(sql`(datetime('now'))`).notNull(),
});

// Reasoning summaries (from Responses API)
export const reasoningSummaries = sqliteTable('reasoning_summaries', {
  id: text('id').primaryKey().$defaultFn(() => crypto.randomUUID()),
  responseId: text('response_id').notNull(),  // OpenAI response ID
  buildId: text('build_id'),
  phase: text('phase'),

  summary: text('summary').notNull(),
  reasoningTokensUsed: integer('reasoning_tokens_used'),
  effortLevel: text('effort_level'),  // 'minimal', 'low', 'medium', 'high', 'xhigh'

  // SYNERGY: Link to Hyper-Thinking run if applicable
  hyperThinkingRunId: text('hyper_thinking_run_id'),

  createdAt: text('created_at').default(sql`(datetime('now'))`).notNull(),
});

// Preserved thinking blocks across turns
export const preservedThinkingBlocks = sqliteTable('preserved_thinking_blocks', {
  id: text('id').primaryKey().$defaultFn(() => crypto.randomUUID()),
  sessionId: text('session_id').notNull(),
  turnNumber: integer('turn_number').notNull(),

  thinkingContent: text('thinking_content').notNull(),
  tokensUsed: integer('tokens_used'),
  phase: text('phase'),

  // SYNERGY: Hyper-Thinking integration
  hyperThinkingPhase: text('hyper_thinking_phase'),  // Which HT phase produced this

  createdAt: text('created_at').default(sql`(datetime('now'))`).notNull(),
});

// =============================================================================
// SHADOW DATA COLLECTION TABLES (for learning)
// =============================================================================

export const shadowIntentSamples = sqliteTable('shadow_intent_samples', {
  id: text('id').primaryKey().$defaultFn(() => crypto.randomUUID()),
  prompt: text('prompt').notNull(),
  generatedContract: text('generated_contract'),
  appSoul: text('app_soul'),
  satisfactionResult: text('satisfaction_result'),
  buildSuccess: integer('build_success', { mode: 'boolean' }),

  // SYNERGY: VL-JEPA embedding reference
  intentEmbeddingId: text('intent_embedding_id'),
  uncertainty: real('uncertainty'),

  createdAt: text('created_at').default(sql`(datetime('now'))`).notNull(),
});

export const shadowVisualSamples = sqliteTable('shadow_visual_samples', {
  id: text('id').primaryKey().$defaultFn(() => crypto.randomUUID()),
  screenshotHash: text('screenshot_hash').notNull(),
  screenshotUrl: text('screenshot_url'),
  designScore: real('design_score'),
  antiSlopScore: real('anti_slop_score'),
  appSoul: text('app_soul'),
  componentType: text('component_type'),

  // SYNERGY: VL-JEPA embedding reference
  visualEmbeddingId: text('visual_embedding_id'),

  createdAt: text('created_at').default(sql`(datetime('now'))`).notNull(),
});
```

2. Update existing tables with embedding references:

```typescript
// Add to buildIntents table
intentEmbeddingId: text('intent_embedding_id'),
visualEmbeddingId: text('visual_embedding_id'),
embeddingModelVersion: text('embedding_model_version'),
hyperThinkingRunId: text('hyper_thinking_run_id'),
hyperThinkingDepth: text('hyper_thinking_depth'),
reasoningChainSummary: text('reasoning_chain_summary'),

// Add to verificationResults table
visualEmbeddingId: text('visual_embedding_id'),
visualSimilarityScore: real('visual_similarity_score'),
embeddingVerified: integer('embedding_verified', { mode: 'boolean' }),
embeddingConfidence: real('embedding_confidence'),

// Add to learnedPatterns table
patternEmbeddingId: text('pattern_embedding_id'),
embeddingModelVersion: text('embedding_model_version'),
clusterId: text('cluster_id'),
clusterCentroidDistance: real('cluster_centroid_distance'),
```

3. Create migration script `server/src/migrations/add-unified-enhancement-tables.ts`

VALIDATION:
- npm run build must pass
- Run migrations successfully
- Verify all new columns/tables exist
- Test relations between tables

DO NOT:
- Modify existing column types (Turso limitation)
- Remove any existing columns
- Change primary key structures
- Modify any auth files or auth tables
```

---

### PHASE 2 PROMPTS: API Power Backbone

---

#### NLP PROMPT 2.1: OpenAI Responses API Integration

```
TASK: Implement OpenAI Responses API for reasoning models in KripTik AI with Hyper-Thinking integration.

CONTEXT:
The Responses API is the NEW standard for agentic tasks (replaces Chat Completions).
Benefits:
- 3% SWE-bench improvement
- 40-80% cache utilization improvement
- Reasoning summaries for transparency
- Preserved reasoning tokens around function calls

CRITICAL SYNERGY: This API powers Hyper-Thinking Phase 5 (Synthesis) with maximum reasoning.

REQUIREMENTS:

1. Create `server/src/services/ai/openai-responses-client.ts`:

```typescript
/**
 * OpenAI Responses API Client
 *
 * Implements the new Responses API for o3, o3-pro, and o4-mini models.
 * Provides reasoning summaries and preserved reasoning for tool calls.
 *
 * SYNERGY: Powers Hyper-Thinking synthesis and critical decisions.
 */

import OpenAI from 'openai';

export interface ResponsesAPIConfig {
  model: 'o3' | 'o3-pro' | 'o4-mini';
  reasoning: {
    effort: 'minimal' | 'low' | 'medium' | 'high' | 'xhigh';
    summary: 'auto' | 'none';
  };
  max_output_tokens: number;
  tools?: ToolDefinition[];
  previous_response_id?: string;  // For context continuity
}

export interface ResponsesAPIResult {
  responseId: string;
  output: string;
  reasoningSummary?: string;  // From reasoning.summary: 'auto'
  reasoningTokensUsed: number;
  outputTokensUsed: number;
  totalTokensUsed: number;
  functionCalls?: FunctionCallWithReasoning[];
}

export interface FunctionCallWithReasoning {
  name: string;
  arguments: Record<string, any>;
  reasoningBeforeCall?: string;  // Preserved reasoning before this call
}

export class OpenAIResponsesClient {
  private client: OpenAI;

  constructor() {
    this.client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
  }

  async create(config: ResponsesAPIConfig, input: string): Promise<ResponsesAPIResult> {
    // Implementation using new Responses API
    const response = await this.client.responses.create({
      model: config.model,
      input,
      reasoning: config.reasoning,
      max_output_tokens: config.max_output_tokens,
      tools: config.tools,
      previous_response_id: config.previous_response_id,
    });

    // Extract reasoning summary
    const reasoningSummary = response.output.find(
      item => item.type === 'reasoning' && item.summary
    )?.summary;

    // Return structured result
    return {
      responseId: response.id,
      output: this.extractOutput(response),
      reasoningSummary,
      reasoningTokensUsed: response.usage.reasoning_tokens,
      outputTokensUsed: response.usage.output_tokens,
      totalTokensUsed: response.usage.total_tokens,
      functionCalls: this.extractFunctionCalls(response),
    };
  }

  /**
   * For Hyper-Thinking Synthesis - uses xhigh effort
   */
  async synthesize(
    explorations: string[],
    critiques: string[],
    originalTask: string,
    previousResponseId?: string
  ): Promise<ResponsesAPIResult> {
    const input = this.formatSynthesisPrompt(explorations, critiques, originalTask);

    return this.create({
      model: 'o3',
      reasoning: { effort: 'xhigh', summary: 'auto' },  // Maximum effort for synthesis
      max_output_tokens: 16000,
      previous_response_id: previousResponseId,
    }, input);
  }

  /**
   * For Intent Satisfaction verification - uses xhigh effort
   */
  async verifyIntentSatisfaction(
    intent: string,
    currentOutput: string,
    previousResponseId?: string
  ): Promise<ResponsesAPIResult> {
    const input = this.formatVerificationPrompt(intent, currentOutput);

    return this.create({
      model: 'o3',
      reasoning: { effort: 'xhigh', summary: 'auto' },
      max_output_tokens: 8000,
      previous_response_id: previousResponseId,
    }, input);
  }

  // Helper methods...
}

export function getOpenAIResponsesClient(): OpenAIResponsesClient;
```

2. Create `server/src/services/ai/openai-responses-types.ts`:
   - Full type definitions for Responses API
   - ReasoningItem interface
   - FunctionCallWithReasoning interface

3. Modify `server/src/services/ai/openrouter-client.ts`:
   - Add RESPONSES_API_MODELS constant
   - Update PHASE_CONFIGS to use Responses API for o3/o4-mini
   - Add getResponsesClient() factory function

4. Integrate with Hyper-Thinking:
   - Phase 5 (Synthesis) uses Responses API with xhigh effort
   - Reasoning summaries stored for learning
   - Previous response ID passed between phases for context continuity

PHASE CONFIG UPDATES:

```typescript
'intent_satisfaction': {
    model: ANTHROPIC_MODELS.OPUS_4_5,
    provider: 'anthropic',
    effort: 'high',
    thinkingBudget: 64000,
    // NEW: Use Responses API for verification
    verificationModel: OPENAI_MODELS.O3,
    verificationProvider: 'openai_responses',
    verificationEffort: 'xhigh',
    verificationSummary: 'auto',
},

'hyper_thinking_synthesis': {
    model: OPENAI_MODELS.O3,
    provider: 'openai_responses',
    effort: 'xhigh',
    summary: 'auto',
    description: 'Maximum reasoning for synthesis phase',
},
```

VALIDATION:
- npm run build must pass
- Test with Hyper-Thinking synthesis phase
- Verify reasoning summaries are captured
- Test previous_response_id context continuity
- Store reasoning summaries in reasoningSummaries table

DO NOT MODIFY: Auth files
```

---

#### NLP PROMPT 2.2: Interleaved Thinking Beta

```
TASK: Implement Anthropic Interleaved Thinking Beta for multi-step tool use with Hyper-Thinking integration.

CONTEXT:
Interleaved Thinking (anthropic-beta: interleaved-thinking-2025-05-14) allows Claude to:
- Think BETWEEN tool calls (not just before)
- Have budget_tokens exceed max_tokens
- Better reasoning for multi-step operations

CRITICAL SYNERGY: This powers Hyper-Thinking Phase 3 (Parallel Exploration) and Phase 4 (Critique).

REQUIREMENTS:

1. Modify `server/src/services/ai/openrouter-client.ts`:
   - Add INTERLEAVED_THINKING_2025 to OPENROUTER_BETAS
   - Update getBetaHeaders() to include interleaved thinking

2. Modify `server/src/services/ai/claude-service.ts`:

```typescript
interface GenerateOptions {
  // ... existing options

  // NEW: Interleaved Thinking
  interleavedThinking?: boolean;
  thinkingBudget?: number;  // Can exceed max_tokens with interleaved
}

async generate(prompt: string, options: GenerateOptions): Promise<GenerateResult> {
  const headers: Record<string, string> = {};

  if (options.interleavedThinking) {
    headers['anthropic-beta'] = 'interleaved-thinking-2025-05-14';
  }

  const response = await client.messages.create({
    model: options.model,
    messages: [{ role: 'user', content: prompt }],
    max_tokens: options.maxTokens,
    thinking: options.interleavedThinking ? {
      type: 'enabled',
      budget_tokens: options.thinkingBudget || 64000,  // Can exceed max_tokens
    } : undefined,
    tools: options.tools,
  }, { headers });

  // Parse interleaved thinking blocks
  const thinkingBlocks = this.parseInterleavedThinking(response);

  return {
    content: this.extractContent(response),
    thinkingBlocks,  // Array of thinking between tool calls
    usage: response.usage,
  };
}
```

3. Create `server/src/services/ai/interleaved-thinking-handler.ts`:
   - Parse thinking blocks between tool calls
   - Aggregate total thinking tokens across interleaved blocks
   - Provide reasoning trace for Hyper-Thinking learning

4. Integrate with Hyper-Thinking phases:

```typescript
// Phase 3: Parallel Exploration with Interleaved Thinking
async runParallelExploration(task, decomposition, priorKnowledge, config) {
  const explorations = await Promise.all([
    this.exploreWithInterleaved(task, 'approach_1', { temperature: 0.7 }),
    this.exploreWithInterleaved(task, 'approach_2', { temperature: 0.5 }),
    this.exploreWithInterleaved(task, 'approach_3', { temperature: 0.3 }),
  ]);

  return explorations;
}

private async exploreWithInterleaved(task, approach, options) {
  const result = await this.claudeService.generate(
    this.formatExplorationPrompt(task, approach),
    {
      model: 'claude-opus-4-5-20251101',
      interleavedThinking: true,
      thinkingBudget: 32000,  // Budget for thinking between tool calls
      tools: this.getExplorationTools(),
      temperature: options.temperature,
    }
  );

  // Capture thinking blocks for learning
  await this.captureInterleavedThinking(result.thinkingBlocks);

  return result;
}

// Phase 4: Adversarial Critique with Interleaved Thinking
async runAdversarialCritique(explorations, taskType) {
  const result = await this.claudeService.generate(
    this.formatCritiquePrompt(explorations),
    {
      model: 'claude-opus-4-5-20251101',
      interleavedThinking: true,
      thinkingBudget: 24000,
      tools: [
        { name: 'analyze_security', ... },
        { name: 'check_performance', ... },
        { name: 'verify_completeness', ... },
      ],
    }
  );

  // Interleaved thinking shows reasoning BETWEEN each analysis tool call
  return this.parseCritiqueResults(result);
}
```

PHASES TO ENABLE INTERLEAVED THINKING:
- build_agent_complex (multi-step architecture changes)
- error_level_3 (component rewrite with tool use)
- error_level_4 (full feature rebuild)
- hyper_thinking_exploration (parallel exploration)
- hyper_thinking_critique (adversarial critique)

VALIDATION:
- npm run build must pass
- Test with multi-step tool-use scenario
- Verify thinking blocks appear between tool calls
- Capture thinking blocks in preservedThinkingBlocks table

DO NOT MODIFY: Auth files
```

---

#### NLP PROMPT 2.3: Deliberative Alignment Quality Specs

```
TASK: Implement Deliberative Alignment for code quality with Hyper-Thinking Phase 4 integration.

CONTEXT:
Deliberative Alignment reduces problematic outputs from 8.7% to 0.3% by:
1. Teaching models explicit quality specifications
2. Having them reason over specs before generation
3. Verifying compliance in the output

CRITICAL SYNERGY: This powers Hyper-Thinking Phase 4 (Adversarial Critique).

REQUIREMENTS:

1. Create `server/src/services/ai/quality-specs.ts`:

```typescript
/**
 * KripTik Code Quality Specifications
 *
 * These specs are injected into system prompts and used by
 * Hyper-Thinking Phase 4 (Adversarial Critique) to verify compliance.
 */

export interface QualitySpec {
  id: string;
  name: string;
  rule: string;
  severity: 'blocking' | 'major' | 'minor';
  examples?: {
    violation: string;
    correct: string;
  };
  critiqueQuestion?: string;  // For Hyper-Thinking critique
}

export const CODE_QUALITY_SPECS: QualitySpec[] = [
  {
    id: 'no-placeholders',
    name: 'No Placeholder Content',
    rule: 'NEVER use TODO, FIXME, lorem ipsum, "Coming soon", mock data, or placeholder comments',
    severity: 'blocking',
    examples: {
      violation: 'function getData() { /* TODO: implement */ return []; }',
      correct: 'async function getData() { return await db.select().from(items); }',
    },
    critiqueQuestion: 'Does this code contain any TODO, FIXME, or placeholder content?',
  },
  {
    id: 'complete-implementations',
    name: 'Complete Implementations',
    rule: 'Every function must be fully implemented, no stubs or partial code',
    severity: 'blocking',
    critiqueQuestion: 'Are all functions fully implemented with real logic?',
  },
  {
    id: 'proper-error-handling',
    name: 'Proper Error Handling',
    rule: 'All async operations must have try/catch, all errors must be handled meaningfully',
    severity: 'major',
    critiqueQuestion: 'Are all async operations wrapped in try/catch with meaningful error handling?',
  },
  {
    id: 'no-exposed-secrets',
    name: 'No Exposed Secrets',
    rule: 'Never hardcode API keys, passwords, or secrets. Use environment variables.',
    severity: 'blocking',
    critiqueQuestion: 'Are there any hardcoded secrets or API keys?',
  },
  {
    id: 'proper-input-validation',
    name: 'Proper Input Validation',
    rule: 'All user inputs must be validated before use',
    severity: 'major',
    critiqueQuestion: 'Is all user input properly validated?',
  },
  {
    id: 'anti-slop-design',
    name: 'Anti-Slop Design',
    rule: 'No purple-pink gradients, no flat designs, use depth and premium aesthetics',
    severity: 'major',
    critiqueQuestion: 'Does the design avoid common AI slop patterns (flat, purple-pink, emoji)?',
  },
  {
    id: 'custom-icons',
    name: 'Custom Icons Only',
    rule: 'Use src/components/icons/ custom icons, never Lucide or other generic icon libraries',
    severity: 'minor',
    critiqueQuestion: 'Are custom icons used instead of generic icon libraries?',
  },
  {
    id: 'integration-complete',
    name: 'Integration Complete',
    rule: 'All components must be wired up and integrated, no orphan code',
    severity: 'blocking',
    critiqueQuestion: 'Are all components properly integrated with no orphan code?',
  },
  {
    id: 'type-safety',
    name: 'Type Safety',
    rule: 'Use proper TypeScript types, avoid any, define interfaces for all data structures',
    severity: 'major',
    critiqueQuestion: 'Is the code properly typed with no "any" types?',
  },
  {
    id: 'responsive-design',
    name: 'Responsive Design',
    rule: 'All UI must be responsive with mobile-first approach',
    severity: 'major',
    critiqueQuestion: 'Is the UI responsive and mobile-friendly?',
  },
];

// Get specs by severity for phased enforcement
export function getBlockingSpecs(): QualitySpec[] {
  return CODE_QUALITY_SPECS.filter(s => s.severity === 'blocking');
}

export function getCritiqueQuestions(taskType: string): string[] {
  return CODE_QUALITY_SPECS.map(s => s.critiqueQuestion).filter(Boolean) as string[];
}
```

2. Create `server/src/services/ai/deliberative-alignment.ts`:

```typescript
/**
 * Deliberative Alignment Service
 *
 * Injects quality specs into prompts and verifies compliance.
 * Integrates with Hyper-Thinking Phase 4 for automated critique.
 */

import { CODE_QUALITY_SPECS, QualitySpec, getBlockingSpecs, getCritiqueQuestions } from './quality-specs.js';

export class DeliberativeAlignmentService {
  /**
   * Inject quality specs into system prompt
   */
  injectSpecs(systemPrompt: string, taskType: string): string {
    const specs = this.getRelevantSpecs(taskType);
    const specsBlock = this.formatSpecsForPrompt(specs);

    return `${systemPrompt}

## Code Quality Specifications (MUST FOLLOW)

Before generating ANY code, you MUST:
1. Review each specification below
2. Explicitly reason about how your code will comply
3. Verify compliance before outputting

${specsBlock}

CRITICAL: If you cannot comply with a BLOCKING specification, explain why and propose an alternative.`;
  }

  /**
   * Generate critique questions for Hyper-Thinking Phase 4
   */
  getCritiquePrompt(explorations: string[], taskType: string): string {
    const questions = getCritiqueQuestions(taskType);

    return `You are a senior engineer reviewing code for production readiness.

## Explorations to Review
${explorations.map((e, i) => `### Approach ${i + 1}\n${e}`).join('\n\n')}

## Quality Verification Questions
For each exploration, answer these questions:

${questions.map((q, i) => `${i + 1}. ${q}`).join('\n')}

## Output Format
For each question, provide:
- Answer: Yes/No/Partial
- Finding: What you observed
- Severity: blocking/major/minor/none
- Suggestion: How to fix (if applicable)

Be skeptical. Look for subtle issues. Better to catch problems now than in production.`;
  }

  /**
   * Verify spec compliance in generated output
   */
  async verifyCompliance(output: string, specs: QualitySpec[]): Promise<{
    compliant: boolean;
    violations: Array<{ spec: QualitySpec; issue: string }>;
  }> {
    // Implementation: Check output against each spec
    // Use pattern matching and heuristics
  }

  private formatSpecsForPrompt(specs: QualitySpec[]): string {
    return specs.map(spec => `
### ${spec.name} (${spec.severity.toUpperCase()})
${spec.rule}
${spec.examples ? `
**Violation Example:**
\`\`\`
${spec.examples.violation}
\`\`\`

**Correct Example:**
\`\`\`
${spec.examples.correct}
\`\`\`
` : ''}`).join('\n');
  }

  private getRelevantSpecs(taskType: string): QualitySpec[] {
    // All specs apply, but order by relevance to task type
    return CODE_QUALITY_SPECS;
  }
}

export function getDeliberativeAlignmentService(): DeliberativeAlignmentService;
```

3. Integrate with Hyper-Thinking Phase 4:

```typescript
// In hyper-thinking-engine.ts Phase 4
async runAdversarialCritique(explorations: ExplorationTrack[], taskType: TaskType): Promise<CritiqueResult[]> {
  const alignmentService = getDeliberativeAlignmentService();

  // Generate critique prompt with Deliberative Alignment questions
  const critiquePrompt = alignmentService.getCritiquePrompt(
    explorations.map(e => e.output),
    taskType
  );

  const response = await this.claudeService.generate(critiquePrompt, {
    model: 'claude-opus-4-5-20251101',
    interleavedThinking: true,
    thinkingBudget: 24000,
    temperature: 0.2,  // Lower temp for skeptical analysis
  });

  // Parse critique results
  const critiques = this.parseCritiqueResults(response.content);

  // Store effective critiques for learning
  await this.storeEffectiveCritiques(critiques);

  return critiques;
}
```

4. Integrate with claude-service.ts:
   - Inject specs into system prompt for code generation phases
   - Parse spec compliance from reasoning blocks
   - Score spec adherence in responses

VALIDATION:
- npm run build must pass
- Test with known "slop" patterns, verify rejection
- Test Hyper-Thinking Phase 4 uses critique questions
- Measure reduction in placeholder content

DO NOT MODIFY: Auth files
```

---

### PHASE 3 PROMPTS: Cognitive Enhancement (Hyper-Thinking)

---

#### NLP PROMPT 3.1: Hyper-Thinking Engine with VL-JEPA Integration

```
TASK: Create the Hyper-Thinking Engine with VL-JEPA semantic awareness and Model Enhancement power.

CONTEXT:
The Hyper-Thinking Engine is a 6-phase cognitive pipeline that makes ANY AI model produce higher quality outputs. This implementation INTEGRATES with:
1. VL-JEPA: Semantic embeddings for decomposition and pattern matching
2. Model Enhancements: Responses API, Interleaved Thinking, xhigh effort
3. Deliberative Alignment: Quality specs for critique phase

REQUIREMENTS:

1. Create `server/src/services/ai/hyper-thinking/hyper-thinking-engine.ts`:

```typescript
/**
 * Hyper-Thinking Engine
 *
 * A cognitive enhancement system that improves any AI model's output
 * through structured thinking phases with semantic awareness.
 *
 * SYNERGY INTEGRATIONS:
 * - VL-JEPA: Semantic embeddings for decomposition and pattern matching
 * - Responses API: xhigh effort for synthesis phase
 * - Interleaved Thinking: Multi-step reasoning in exploration/critique
 * - Deliberative Alignment: Quality specs in critique phase
 * - Component 28: Pattern storage and retrieval
 *
 * Modes:
 * - Shadow: Runs in parallel, collects data, doesn't block
 * - Active: Full pipeline, blocks until complete
 * - Accelerated: Uses embedding-based pattern shortcuts
 */

import { EventEmitter } from 'events';
import { v4 as uuidv4 } from 'uuid';
import { db } from '../../../../db.js';
import { hyperThinkingRuns } from '../../../../schema.js';
import { HyperThinkingModeController } from './mode-controller.js';
import { HyperThinkingTriggerClassifier } from './trigger-classifier.js';
import { getClaudeService } from '../claude-service.js';
import { getOpenAIResponsesClient } from '../openai-responses-client.js';
import { getVLJEPAService } from '../../vljepa/index.js';
import { getEmbeddingService } from '../../embeddings/index.js';
import { getQdrantClient } from '../../embeddings/qdrant-client.js';
import { getDeliberativeAlignmentService } from '../deliberative-alignment.js';
import type {
  HyperThinkingConfig,
  HyperThinkingResult,
  HyperThinkingMode,
  TaskType,
  DecompositionResult,
  ExplorationTrack,
  CritiqueResult,
  SynthesisResult,
} from './types.js';

const DEFAULT_CONFIG: HyperThinkingConfig = {
  mode: 'shadow',
  depth: 'standard',
  model: 'claude-sonnet-4-5-20241022',
  enableParallelExploration: true,
  explorationTracks: 3,
  enableCritique: true,
  enableSynthesis: true,
  maxTokens: 32000,
  timeoutMs: 60000,
  // SYNERGY OPTIONS
  useVLJEPADecomposition: true,     // Use semantic embeddings for decomposition
  useEmbeddingPatterns: true,       // Use embedding similarity for pattern shortcuts
  useInterleavedThinking: true,     // Use Interleaved Thinking in exploration/critique
  useResponsesAPISynthesis: true,   // Use o3 xhigh for synthesis
  useDeliberativeCritique: true,    // Use Deliberative Alignment in critique
};

export class HyperThinkingEngine extends EventEmitter {
  private modeController: HyperThinkingModeController;
  private triggerClassifier: HyperThinkingTriggerClassifier;
  private claudeService: ReturnType<typeof getClaudeService>;
  private responsesClient: ReturnType<typeof getOpenAIResponsesClient>;
  private vljepaService: ReturnType<typeof getVLJEPAService>;
  private embeddingService: ReturnType<typeof getEmbeddingService>;
  private qdrantClient: ReturnType<typeof getQdrantClient>;
  private alignmentService: ReturnType<typeof getDeliberativeAlignmentService>;

  constructor() {
    super();
    this.modeController = new HyperThinkingModeController();
    this.triggerClassifier = new HyperThinkingTriggerClassifier();
    this.claudeService = getClaudeService();
    this.responsesClient = getOpenAIResponsesClient();
    this.vljepaService = getVLJEPAService();
    this.embeddingService = getEmbeddingService();
    this.qdrantClient = getQdrantClient();
    this.alignmentService = getDeliberativeAlignmentService();
  }

  /**
   * Main entry point - process a task through Hyper-Thinking
   */
  async process(
    task: string,
    taskType: TaskType,
    options?: Partial<HyperThinkingConfig>
  ): Promise<HyperThinkingResult> {
    const config = { ...DEFAULT_CONFIG, ...options };
    const runId = `ht_${uuidv4()}`;
    const startTime = Date.now();

    // SYNERGY: Get VL-JEPA semantic embedding for the task
    let intentEmbedding;
    if (config.useVLJEPADecomposition) {
      intentEmbedding = await this.vljepaService.embedIntent(task);
    }

    // Classify the trigger with semantic awareness
    const trigger = await this.triggerClassifier.classify({
      taskType,
      prompt: task,
      complexity: this.triggerClassifier.estimateComplexity(task),
      model: config.model,
      semanticUncertainty: intentEmbedding?.uncertainty,
    });

    // Get optimal mode
    const modeDecision = await this.modeController.getOptimalMode(taskType);
    const mode = trigger.runInShadowMode ? 'shadow' : modeDecision.mode;

    this.emit('process_started', { runId, taskType, mode, hasSemanticContext: !!intentEmbedding });

    // SYNERGY: Check for embedding-based pattern shortcuts
    if (config.useEmbeddingPatterns && trigger.canUsePatternShortcut && modeDecision.canUsePatternShortcuts) {
      const patternMatch = await this.findPatternByEmbedding(task, taskType, intentEmbedding);
      if (patternMatch && patternMatch.confidence > 0.9) {
        return this.runWithPatternShortcut(runId, task, taskType, config, patternMatch);
      }
    }

    // Shadow mode: run in parallel
    if (mode === 'shadow') {
      return this.runShadowMode(runId, task, taskType, config, intentEmbedding);
    }

    // Full pipeline
    return this.runFullPipeline(runId, task, taskType, config, mode, intentEmbedding);
  }

  /**
   * Full 6-phase pipeline with all synergy integrations
   */
  private async runFullPipeline(
    runId: string,
    task: string,
    taskType: TaskType,
    config: HyperThinkingConfig,
    mode: HyperThinkingMode,
    intentEmbedding?: any
  ): Promise<HyperThinkingResult> {
    const startTime = Date.now();
    let totalTokens = 0;

    // Phase 1: DECOMPOSITION (with VL-JEPA semantic awareness)
    this.emit('phase_started', { runId, phase: 'decomposition' });
    const decomposition = await this.runDecomposition(task, taskType, config, intentEmbedding);
    totalTokens += decomposition.tokensUsed;
    this.emit('phase_completed', { runId, phase: 'decomposition' });

    // Phase 2: PRIOR KNOWLEDGE INJECTION (embedding-based retrieval)
    this.emit('phase_started', { runId, phase: 'prior_knowledge' });
    const priorKnowledge = await this.loadPriorKnowledgeByEmbedding(
      task, taskType, decomposition.result, intentEmbedding
    );
    this.emit('phase_completed', { runId, phase: 'prior_knowledge' });

    // Phase 3: PARALLEL EXPLORATION (with Interleaved Thinking)
    this.emit('phase_started', { runId, phase: 'exploration' });
    const explorations = await this.runParallelExplorationWithInterleaved(
      task, decomposition.result, priorKnowledge, config
    );
    totalTokens += explorations.reduce((sum, e) => sum + e.tokensUsed, 0);
    this.emit('phase_completed', { runId, phase: 'exploration' });

    // Phase 4: ADVERSARIAL CRITIQUE (with Deliberative Alignment)
    let critiques: CritiqueResult[] = [];
    if (config.enableCritique) {
      this.emit('phase_started', { runId, phase: 'critique' });
      critiques = await this.runCritiqueWithDeliberativeAlignment(explorations, taskType);
      this.emit('phase_completed', { runId, phase: 'critique' });
    }

    // Phase 5: SYNTHESIS (with Responses API xhigh effort)
    let synthesis: SynthesisResult;
    if (config.enableSynthesis && config.useResponsesAPISynthesis) {
      this.emit('phase_started', { runId, phase: 'synthesis' });
      synthesis = await this.runSynthesisWithResponsesAPI(explorations, critiques, task);
      totalTokens += synthesis.tokensUsed;
      this.emit('phase_completed', { runId, phase: 'synthesis' });
    } else {
      // Fallback: use best exploration
      const bestExploration = this.selectBestExploration(explorations);
      synthesis = {
        output: bestExploration.output,
        approachUsed: bestExploration.approach,
        critiquesAddressed: [],
        qualityScore: 0,
        tokensUsed: 0,
        reasoningSummary: '',
      };
    }

    // Phase 6: VERIFICATION BRIDGE (with VL-JEPA visual verification)
    this.emit('phase_started', { runId, phase: 'verification' });
    const qualityScore = await this.runVerificationWithVLJEPA(
      synthesis.output, task, critiques, intentEmbedding
    );
    this.emit('phase_completed', { runId, phase: 'verification' });

    // Store run with all synergy data
    await this.storeRun(runId, {
      taskType,
      mode,
      decomposition: decomposition.result,
      explorations,
      critiques,
      synthesis,
      tokensUsed: totalTokens,
      durationMs: Date.now() - startTime,
      qualityScore,
      intentEmbeddingId: intentEmbedding?.id,
      usedSynergies: {
        vljepa: config.useVLJEPADecomposition,
        embeddingPatterns: config.useEmbeddingPatterns,
        interleaved: config.useInterleavedThinking,
        responsesAPI: config.useResponsesAPISynthesis,
        deliberative: config.useDeliberativeCritique,
      },
    });

    this.emit('process_completed', { runId, qualityScore });

    return {
      runId,
      mode,
      phases: {
        decomposition: decomposition.result,
        explorations,
        critiques,
        synthesis,
      },
      output: synthesis.output,
      tokensUsed: totalTokens,
      durationMs: Date.now() - startTime,
      qualityScore,
      qualityImprovement: 0,
      reasoningSummary: synthesis.reasoningSummary,
    };
  }

  // =========================================================================
  // SYNERGY-ENHANCED PHASE IMPLEMENTATIONS
  // =========================================================================

  /**
   * Phase 1: Decomposition with VL-JEPA semantic analysis
   */
  private async runDecomposition(
    task: string,
    taskType: TaskType,
    config: HyperThinkingConfig,
    intentEmbedding?: any
  ): Promise<{ result: DecompositionResult; tokensUsed: number }> {
    // Use VL-JEPA semantic components for smarter decomposition
    const semanticContext = intentEmbedding ? `
## Semantic Analysis (VL-JEPA)
- Action intent: ${intentEmbedding.semanticComponents?.action || 'unknown'}
- Target intent: ${intentEmbedding.semanticComponents?.target || 'unknown'}
- Constraints: ${intentEmbedding.semanticComponents?.constraints?.join(', ') || 'none'}
- Uncertainty: ${intentEmbedding.uncertainty || 0}
` : '';

    const decompositionPrompt = `You are a cognitive analyst breaking down a complex problem.

## Task
${task}

## Task Type
${taskType}

${semanticContext}

## Instructions
1. Identify 2-5 distinct sub-problems
2. For each sub-problem:
   - Give it a unique ID (sp_1, sp_2, etc.)
   - Classify its type
   - Describe what needs to be solved
   - List dependencies on other sub-problems
3. Estimate overall complexity (1-10)
4. List what prior knowledge would help

Output JSON format...`;

    const response = await this.claudeService.generate(decompositionPrompt, {
      model: 'claude-haiku-3-5-20241022',
      maxTokens: 2000,
      temperature: 0.3,
    });

    // Store decomposition embedding for pattern matching
    if (config.useEmbeddingPatterns) {
      const decompositionEmbedding = await this.embeddingService.embedText(
        JSON.stringify(response.content),
        { purpose: 'decomposition' }
      );
      await this.qdrantClient.upsert('decomposition_patterns', {
        vector: decompositionEmbedding,
        payload: { taskType, task, runId: uuidv4() },
      });
    }

    return {
      result: this.parseDecomposition(response.content),
      tokensUsed: response.usage?.total_tokens || 0,
    };
  }

  /**
   * Phase 2: Prior Knowledge with embedding-based retrieval (50x faster)
   */
  private async loadPriorKnowledgeByEmbedding(
    task: string,
    taskType: TaskType,
    decomposition: DecompositionResult,
    intentEmbedding?: any
  ): Promise<any> {
    // Use embedding similarity for instant pattern retrieval
    const taskEmbedding = intentEmbedding?.embedding ||
      await this.embeddingService.embedText(task, { purpose: 'pattern_match' });

    // Search code patterns by embedding similarity
    const patterns = await this.qdrantClient.search('code_pattern_embeddings', {
      vector: taskEmbedding,
      filter: { taskType },
      limit: 5,
      with_payload: true,
    });

    // Search reasoning skeletons by embedding similarity
    const skeletons = await this.qdrantClient.search('reasoning_skeletons', {
      vector: taskEmbedding,
      filter: { taskType },
      limit: 1,
      with_payload: true,
    });

    return {
      patterns: patterns.map(p => p.payload),
      skeleton: skeletons[0]?.payload || null,
      retrievalMethod: 'embedding_similarity',
    };
  }

  /**
   * Phase 3: Parallel Exploration with Interleaved Thinking
   */
  private async runParallelExplorationWithInterleaved(
    task: string,
    decomposition: DecompositionResult,
    priorKnowledge: any,
    config: HyperThinkingConfig
  ): Promise<ExplorationTrack[]> {
    const explorationPromises = [0.7, 0.5, 0.3].map(async (temperature, index) => {
      const approach = `approach_${index + 1}`;

      const result = await this.claudeService.generate(
        this.formatExplorationPrompt(task, decomposition, priorKnowledge, approach),
        {
          model: 'claude-opus-4-5-20251101',
          interleavedThinking: config.useInterleavedThinking,
          thinkingBudget: 32000,
          maxTokens: 8000,
          temperature,
          tools: this.getExplorationTools(),
        }
      );

      return {
        trackId: `track_${index + 1}`,
        approach,
        temperature,
        output: result.content,
        tokensUsed: result.usage?.total_tokens || 0,
        thinkingBlocks: result.thinkingBlocks,
        earlyTerminated: false,
      };
    });

    // Race for speed, collect all for comparison
    const explorations = await Promise.all(explorationPromises);

    // Store interleaved thinking for learning
    for (const exp of explorations) {
      if (exp.thinkingBlocks?.length) {
        await this.storeThinkingBlocks(exp.thinkingBlocks, 'exploration');
      }
    }

    return explorations;
  }

  /**
   * Phase 4: Adversarial Critique with Deliberative Alignment
   */
  private async runCritiqueWithDeliberativeAlignment(
    explorations: ExplorationTrack[],
    taskType: TaskType
  ): Promise<CritiqueResult[]> {
    // Get critique prompt with Deliberative Alignment questions
    const critiquePrompt = this.alignmentService.getCritiquePrompt(
      explorations.map(e => e.output),
      taskType
    );

    const response = await this.claudeService.generate(critiquePrompt, {
      model: 'claude-opus-4-5-20251101',
      interleavedThinking: true,
      thinkingBudget: 24000,
      maxTokens: 4000,
      temperature: 0.2,
    });

    const critiques = this.parseCritiqueResults(response.content);

    // Store effective critiques in library
    for (const critique of critiques) {
      if (critique.severity !== 'none') {
        await this.storeCritique(critique, taskType);
      }
    }

    return critiques;
  }

  /**
   * Phase 5: Synthesis with Responses API xhigh effort
   */
  private async runSynthesisWithResponsesAPI(
    explorations: ExplorationTrack[],
    critiques: CritiqueResult[],
    originalTask: string
  ): Promise<SynthesisResult> {
    const result = await this.responsesClient.synthesize(
      explorations.map(e => e.output),
      critiques.map(c => `${c.question}: ${c.finding}`),
      originalTask
    );

    return {
      output: result.output,
      approachUsed: 'synthesized',
      critiquesAddressed: critiques.filter(c => c.ledToChange).map(c => c.question),
      qualityScore: 0,
      tokensUsed: result.totalTokensUsed,
      reasoningSummary: result.reasoningSummary || '',
    };
  }

  /**
   * Phase 6: Verification with VL-JEPA visual matching
   */
  private async runVerificationWithVLJEPA(
    output: string,
    task: string,
    critiques: CritiqueResult[],
    intentEmbedding?: any
  ): Promise<number> {
    // If no blocking critiques, quick pass
    const blockingCritiques = critiques.filter(c => c.severity === 'critical');
    if (blockingCritiques.length > 0) {
      return 0.3; // Failed
    }

    // VL-JEPA semantic match (if available)
    if (intentEmbedding) {
      const outputEmbedding = await this.vljepaService.embedIntent(output);
      const similarity = await this.vljepaService.compareIntents(
        intentEmbedding,
        outputEmbedding
      );
      return similarity.score;
    }

    // Fallback: heuristic quality check
    return 0.8;
  }

  // Helper methods...
}

export function getHyperThinkingEngine(): HyperThinkingEngine;
```

2. Create supporting files:
   - `mode-controller.ts` - Mode switching logic
   - `trigger-classifier.ts` - When to activate
   - `types.ts` - Type definitions

3. Create phase implementation files in `phases/`:
   - `decomposition.ts`
   - `prior-knowledge-injector.ts`
   - `parallel-explorer.ts`
   - `adversarial-critique.ts`
   - `synthesis.ts`
   - `verification-bridge.ts`

4. Create learning files in `learning/`:
   - `hyper-thinking-learning.ts`
   - `decomposition-patterns.ts`
   - `critique-library.ts`
   - `reasoning-skeletons.ts`

VALIDATION:
- npm run build must pass
- Test shadow mode doesn't block
- Test all synergy integrations work
- Measure quality improvement

DO NOT MODIFY: Auth files
```

---

### PHASE 4 PROMPTS: VL-JEPA Visual Verification

---

#### NLP PROMPT 4.1: VL-JEPA Service with Semantic Understanding

```
TASK: Create the VL-JEPA service for semantic understanding of intents and visuals.

CONTEXT:
VL-JEPA (Vision-Language Joint Embedding Predictive Architecture) predicts abstract embeddings rather than tokens. This enables:
1. Semantic understanding of intent (not just keywords)
2. Visual understanding of design (not just pixels)
3. Uncertainty quantification (know when to ask questions)

CRITICAL SYNERGY: This powers Hyper-Thinking decomposition and verification phases.

REQUIREMENTS:

1. Create `server/src/services/vljepa/vljepa-service.ts`:

```typescript
/**
 * VL-JEPA Service - Semantic Understanding Engine
 *
 * Provides semantic embeddings for:
 * - Intent understanding (user prompts → meaning)
 * - Visual understanding (screenshots → design semantics)
 * - Video understanding (for Clone Mode)
 *
 * SYNERGY INTEGRATIONS:
 * - Hyper-Thinking: Semantic decomposition and pattern matching
 * - Intent Lock: Semantic contracts that can't be "gamed"
 * - Verification Swarm: 50x faster visual verification
 * - Component 28: Embedding-based pattern storage
 */

import { getEmbeddingService } from '../embeddings/index.js';
import { getQdrantClient } from '../embeddings/qdrant-client.js';

export interface IntentEmbedding {
  id: string;
  embedding: number[];
  uncertainty: number;  // 0-1, lower is more confident
  semanticComponents: {
    action: string;     // What to do (verb)
    target: string;     // What to build (noun)
    constraints: string[];  // How (adjectives)
  };
  qdrantPointId?: string;
}

export interface VisualEmbedding {
  id: string;
  embedding: number[];
  uncertainty: number;
  semanticComponents: {
    layout: string;     // Spatial structure
    style: string;      // Visual aesthetics
    components: string[];  // UI elements
  };
  qdrantPointId?: string;
}

export interface SemanticSimilarity {
  score: number;  // 0-1
  confidence: number;  // 0-1
  alignedComponents: string[];
  misalignedComponents: string[];
}

export class VLJEPAService {
  private embeddingService: ReturnType<typeof getEmbeddingService>;
  private qdrantClient: ReturnType<typeof getQdrantClient>;

  constructor() {
    this.embeddingService = getEmbeddingService();
    this.qdrantClient = getQdrantClient();
  }

  /**
   * Create semantic embedding of user intent
   */
  async embedIntent(prompt: string): Promise<IntentEmbedding> {
    // Generate embedding
    const embedding = await this.embeddingService.embedText(prompt, {
      model: 'bge-large-en-v1.5',
      purpose: 'intent',
    });

    // Extract semantic components (action, target, constraints)
    const components = await this.extractSemanticComponents(prompt);

    // Calculate uncertainty based on component clarity
    const uncertainty = this.calculateUncertainty(components);

    const intentEmbedding: IntentEmbedding = {
      id: `intent_${Date.now()}`,
      embedding,
      uncertainty,
      semanticComponents: components,
    };

    // Store in Qdrant for similarity search
    const qdrantPointId = await this.qdrantClient.upsert('intent_embeddings', {
      vector: embedding,
      payload: {
        prompt,
        ...components,
        uncertainty,
        timestamp: new Date().toISOString(),
      },
    });

    intentEmbedding.qdrantPointId = qdrantPointId;

    return intentEmbedding;
  }

  /**
   * Create semantic embedding of visual (screenshot)
   */
  async embedVisual(imageBase64: string): Promise<VisualEmbedding> {
    // Generate CLIP embedding
    const embedding = await this.embeddingService.embedImage(imageBase64, {
      model: 'clip-vit-large-patch14',
    });

    // Extract visual semantic components
    const components = await this.extractVisualComponents(imageBase64);

    const visualEmbedding: VisualEmbedding = {
      id: `visual_${Date.now()}`,
      embedding,
      uncertainty: 0.1,  // Lower uncertainty for visual
      semanticComponents: components,
    };

    // Store in Qdrant
    const qdrantPointId = await this.qdrantClient.upsert('visual_embeddings', {
      vector: embedding,
      payload: {
        ...components,
        timestamp: new Date().toISOString(),
      },
    });

    visualEmbedding.qdrantPointId = qdrantPointId;

    return visualEmbedding;
  }

  /**
   * Compare intent to visual output (for Intent Satisfaction)
   */
  async compareIntentToVisual(
    intent: IntentEmbedding,
    visual: VisualEmbedding
  ): Promise<SemanticSimilarity> {
    // Calculate embedding similarity
    const score = this.cosineSimilarity(intent.embedding, visual.embedding);

    // Compare semantic components
    const { aligned, misaligned } = this.compareComponents(
      intent.semanticComponents,
      visual.semanticComponents
    );

    return {
      score,
      confidence: 1 - (intent.uncertainty + visual.uncertainty) / 2,
      alignedComponents: aligned,
      misalignedComponents: misaligned,
    };
  }

  /**
   * Compare two intents (for pattern matching)
   */
  async compareIntents(
    intent1: IntentEmbedding,
    intent2: IntentEmbedding
  ): Promise<SemanticSimilarity> {
    const score = this.cosineSimilarity(intent1.embedding, intent2.embedding);

    return {
      score,
      confidence: 1 - Math.max(intent1.uncertainty, intent2.uncertainty),
      alignedComponents: [],
      misalignedComponents: [],
    };
  }

  /**
   * Find similar intents in history (for pattern shortcuts)
   */
  async findSimilarIntents(
    embedding: IntentEmbedding,
    limit: number = 5
  ): Promise<Array<{ intent: IntentEmbedding; similarity: number }>> {
    const results = await this.qdrantClient.search('intent_embeddings', {
      vector: embedding.embedding,
      limit,
      with_payload: true,
    });

    return results.map(r => ({
      intent: {
        id: r.id,
        embedding: r.vector,
        uncertainty: r.payload.uncertainty,
        semanticComponents: {
          action: r.payload.action,
          target: r.payload.target,
          constraints: r.payload.constraints,
        },
        qdrantPointId: r.id,
      },
      similarity: r.score,
    }));
  }

  /**
   * Should we ask clarifying questions? (uncertainty-aware)
   */
  shouldAskClarification(uncertainty: number): boolean {
    return uncertainty > 0.4;  // High uncertainty = ask questions
  }

  // Helper methods
  private async extractSemanticComponents(prompt: string): Promise<{
    action: string;
    target: string;
    constraints: string[];
  }> {
    // Use LLM to extract semantic components
    // Or use NLP heuristics for speed
    return {
      action: 'build',
      target: 'application',
      constraints: [],
    };
  }

  private async extractVisualComponents(imageBase64: string): Promise<{
    layout: string;
    style: string;
    components: string[];
  }> {
    // Use vision model to extract components
    return {
      layout: 'standard',
      style: 'modern',
      components: [],
    };
  }

  private calculateUncertainty(components: any): number {
    // Higher uncertainty if components are vague
    if (!components.action || components.action === 'build') return 0.5;
    if (!components.target) return 0.6;
    return 0.2;
  }

  private cosineSimilarity(a: number[], b: number[]): number {
    let dotProduct = 0;
    let normA = 0;
    let normB = 0;
    for (let i = 0; i < a.length; i++) {
      dotProduct += a[i] * b[i];
      normA += a[i] * a[i];
      normB += b[i] * b[i];
    }
    return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
  }

  private compareComponents(intent: any, visual: any): {
    aligned: string[];
    misaligned: string[];
  } {
    // Compare semantic components
    return { aligned: [], misaligned: [] };
  }
}

// Singleton
let vljepaService: VLJEPAService | null = null;

export function getVLJEPAService(): VLJEPAService {
  if (!vljepaService) {
    vljepaService = new VLJEPAService();
  }
  return vljepaService;
}
```

2. Integrate with Intent Lock:
   - Create semantic embedding of intent contract
   - Store embedding for Phase 5 verification
   - Use uncertainty for clarification questions

3. Integrate with Verification Swarm:
   - Visual Verifier uses embedding comparison (50x faster)
   - Anti-Slop detection via embedding distance from known good designs

VALIDATION:
- npm run build must pass
- Test intent embedding generation
- Test visual embedding generation
- Test similarity comparison accuracy

DO NOT MODIFY: Auth files
```

---

### PHASE 6 PROMPTS: KripTik Cloud

---

#### NLP PROMPT 6.1: KripTik Cloud Full-Stack Deployment

```
TASK: Create KripTik Cloud infrastructure for full-stack production deployment with Hyper-Thinking integration.

CONTEXT:
KripTik Cloud enables complete production deployments:
- Frontend: Vercel/Cloudflare Edge
- Backend: AWS ECS Fargate containers
- AI/ML: RunPod/Vast.ai serverless GPUs
- Storage: S3/R2 + Turso + Qdrant
- Auth: Nango OAuth (500+ services)

CRITICAL SYNERGY: Hyper-Thinking makes infrastructure decisions for optimal deployment.

REQUIREMENTS:

1. Create `server/src/services/cloud/kriptik-cloud.ts`:

```typescript
/**
 * KripTik Cloud - Full-Stack Production Deployment
 *
 * Orchestrates deployment across multiple cloud providers.
 *
 * SYNERGY INTEGRATIONS:
 * - Hyper-Thinking: Infrastructure decisions use cognitive pipeline
 * - VL-JEPA: Complexity analysis for resource sizing
 * - Component 28: Learn optimal configurations over time
 */

import { getHyperThinkingEngine } from '../ai/hyper-thinking/index.js';

export interface CloudStack {
  frontend: {
    provider: 'vercel' | 'cloudflare';
    url: string;
    deploymentId: string;
  };
  backend?: {
    provider: 'ecs' | 'runpod' | 'vastai';
    url: string;
    containerId: string;
    resources: {
      cpu: string;
      memory: string;
      gpu?: string;
    };
  };
  storage?: {
    files: { provider: 's3' | 'r2'; bucket: string };
    database: { provider: 'turso'; url: string };
    vectors?: { provider: 'qdrant'; url: string };
  };
  integrations: {
    provider: 'nango';
    connectedServices: string[];
  };
}

export interface StackConfig {
  projectId: string;
  userId: string;
  frontend: {
    framework: 'next' | 'vite' | 'remix';
    buildCommand: string;
  };
  backend?: {
    type: 'container' | 'serverless';
    dockerfile?: string;
    port?: number;
    env?: Record<string, string>;
    requiresGPU?: boolean;
    gpuType?: string;
  };
  storage?: {
    files: boolean;
    database: boolean;
    vectors: boolean;
  };
  integrations?: string[];  // ['stripe', 'twilio', etc.]
}

export class KripTikCloud {
  private hyperThinking: ReturnType<typeof getHyperThinkingEngine>;

  constructor() {
    this.hyperThinking = getHyperThinkingEngine();
  }

  /**
   * Deploy a complete stack with Hyper-Thinking infrastructure decisions
   */
  async deployStack(config: StackConfig): Promise<CloudStack> {
    // Use Hyper-Thinking for infrastructure decisions
    const infraDecision = await this.hyperThinking.process(
      `Determine optimal infrastructure for:
       - Frontend: ${config.frontend.framework}
       - Backend: ${config.backend?.type || 'none'}
       - Requires GPU: ${config.backend?.requiresGPU || false}
       - Integrations: ${config.integrations?.join(', ') || 'none'}

       Consider cost, performance, and scalability.`,
      'architecture_decision'
    );

    // Parse infrastructure decision
    const infraConfig = this.parseInfraDecision(infraDecision.output);

    // Deploy each component
    const stack: CloudStack = {
      frontend: await this.deployFrontend(config, infraConfig),
      integrations: { provider: 'nango', connectedServices: [] },
    };

    if (config.backend) {
      stack.backend = await this.deployBackend(config, infraConfig);
    }

    if (config.storage) {
      stack.storage = await this.setupStorage(config);
    }

    if (config.integrations?.length) {
      stack.integrations = await this.connectIntegrations(config);
    }

    // Wire everything together
    await this.wireStack(stack, config);

    return stack;
  }

  /**
   * Deploy frontend to Vercel/Cloudflare
   */
  private async deployFrontend(
    config: StackConfig,
    infraConfig: any
  ): Promise<CloudStack['frontend']> {
    // Implementation...
    return {
      provider: infraConfig.frontendProvider || 'vercel',
      url: '',
      deploymentId: '',
    };
  }

  /**
   * Deploy backend to ECS Fargate or RunPod
   */
  private async deployBackend(
    config: StackConfig,
    infraConfig: any
  ): Promise<CloudStack['backend']> {
    if (config.backend?.requiresGPU) {
      // Use RunPod serverless for GPU workloads
      return this.deployToRunPod(config);
    } else {
      // Use ECS Fargate for standard containers
      return this.deployToECS(config);
    }
  }

  /**
   * Deploy GPU workload to RunPod serverless
   */
  private async deployToRunPod(config: StackConfig): Promise<CloudStack['backend']> {
    // RunPod serverless deployment
    // December 2025 API
    return {
      provider: 'runpod',
      url: '',
      containerId: '',
      resources: {
        cpu: '4',
        memory: '16GB',
        gpu: config.backend?.gpuType || 'RTX_4090',
      },
    };
  }

  /**
   * Deploy container to AWS ECS Fargate
   */
  private async deployToECS(config: StackConfig): Promise<CloudStack['backend']> {
    // ECS Express Mode (re:Invent 2025)
    return {
      provider: 'ecs',
      url: '',
      containerId: '',
      resources: {
        cpu: '256',
        memory: '512',
      },
    };
  }

  /**
   * Connect integrations via Nango OAuth
   */
  private async connectIntegrations(config: StackConfig): Promise<CloudStack['integrations']> {
    // Nango handles OAuth for 500+ services
    return {
      provider: 'nango',
      connectedServices: config.integrations || [],
    };
  }

  /**
   * Wire all stack components together
   */
  private async wireStack(stack: CloudStack, config: StackConfig): Promise<void> {
    // Set frontend env vars to point to backend
    // Set backend env vars for storage
    // Inject Nango credentials for integrations
  }

  private parseInfraDecision(output: string): any {
    // Parse Hyper-Thinking output
    return {};
  }
}

export function getKripTikCloud(): KripTikCloud;
```

2. Create provider clients:
   - `runpod-client.ts` - RunPod serverless API
   - `ecs-client.ts` - AWS ECS Fargate API
   - `nango-integration.ts` - Nango OAuth API

3. Create API routes `server/src/routes/cloud.ts`:
   - POST /api/cloud/deploy-stack
   - POST /api/cloud/deploy-gpu
   - GET /api/cloud/stack/:projectId
   - DELETE /api/cloud/stack/:projectId

VALIDATION:
- npm run build must pass
- Test ECS deployment with sample container
- Test RunPod endpoint creation
- Test Nango OAuth connection

DO NOT MODIFY: Auth files
```

---

## SUCCESS METRICS

### Unified Metrics Across All Three Plans

| Metric | Current | Target | Measurement |
|--------|---------|--------|-------------|
| Intent understanding accuracy | ~70% | 95%+ | VL-JEPA semantic match score |
| Verification speed | 30-60s | 0.5-1s | VL-JEPA embedding comparison |
| Hyper-Thinking quality boost | N/A | 30-50% | Comparative quality scoring |
| Pattern shortcut activation | N/A | 60%+ | Embedding similarity > 0.9 |
| First-time-right rate | ~60% | 90%+ | Builds without rework |
| Complete deployment rate | ~30% | 95%+ | Full stack deployed and running |
| Context compaction efficiency | N/A | 50%+ | Token reduction with semantic preservation |
| Deliberative spec compliance | N/A | 99%+ | No blocking spec violations |
| Learning acceleration | 1000s builds | 100s builds | Pattern maturity time |

### The Compound Effect

Each system multiplies the others:

```
VL-JEPA Semantic Understanding (10x better intent capture)
  × Hyper-Thinking Cognitive Enhancement (30-50% quality boost)
  × Model Enhancements (xhigh effort + interleaved thinking)
  × Embedding-Based Learning (50x faster pattern matching)
  × KripTik Cloud Deployment (complete production apps)
  ─────────────────────────────────────────────────────────
  = "Holy Shit" User Reaction
  = Production-Ready Apps from Single Prompt
  = The World's Most Capable AI Builder
```

---

## VERIFICATION CHECKLIST

After implementing each prompt:

```
[ ] npm run build passes
[ ] No TypeScript errors
[ ] No auth files modified (auth.ts, middleware/auth.ts, auth-client.ts)
[ ] Synergy integrations work correctly
[ ] Database migrations run successfully
[ ] Qdrant collections created
[ ] Test with relevant build phase
[ ] Measure improvement vs baseline
[ ] Update .claude/rules/01-session-context.md
[ ] Commit with descriptive message
[ ] Push to branch
```

---

## SOURCES

### From implementation-plan-kriptik-ultimate.md
- VL-JEPA architecture and semantic understanding
- KripTik Cloud deployment infrastructure
- Component 28 embedding enhancement
- Qdrant vector database setup

### From implementation-plan-hyper-thinking.md
- 6-phase cognitive pipeline
- Shadow/Active/Accelerated modes
- Pattern learning and shortcuts
- Model equalization

### From implementation-plan-model-enhancements-jan2026.md
- OpenAI Responses API
- Anthropic Interleaved Thinking
- xhigh effort reasoning
- Context compaction
- Deliberative Alignment

### Additional Research
- [VL-JEPA Paper (December 2025)](https://arxiv.org/abs/2512.10942)
- [OpenAI Reasoning Models](https://platform.openai.com/docs/guides/reasoning)
- [Claude Extended Thinking](https://platform.claude.com/docs/en/build-with-claude/extended-thinking)
- [RunPod Serverless](https://docs.runpod.io/serverless/overview)
- [Nango OAuth](https://nango.dev/auth)

---

*This unified implementation plan synthesizes three transformative capability sets into a single, synergistic architecture. Each enhancement amplifies the others, creating compound improvements that make KripTik AI the world's most capable AI builder.*

*Total Implementation Time: 10-12 weeks*
*Expected Result: Complete production apps from single NLP prompts*

*Last Updated: January 2, 2026*
